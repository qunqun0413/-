LR
Liner Regression
注意一句话：多变量线性回归之前必须要Feature Scaling！
1.线性回归
线性回归试图学得：

均方误差是回归任务中最常用的性能度量，因此我们可试图使均方误差最小化，即：

基于均方误差最小化来进行模型求解的方式称为最小二乘法，在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线的欧氏距离之和最小。分别对w和b进行求解



注：因为是线性回归，所以习得的函数是为线性函数，即直线函数
但是实际操作中，给定数据集D，样本有d个属性描述，此时我们试图学得





对于不是满秩矩阵，x的列数多于行数，此时可以求出多个w，它们都能是均方误差最小化，常见的做法是引入正则项。

2.对数几率回归(逻辑回归)

逻辑回归本质上也是线性回归，只不过在特征到结果的映射中加入了一层函数映射--sigmoid函数，即先把特征线性求和，然后再使用sigmoid函数。逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。
逻辑回归的第一个基本假设是假设数据服从伯努利分布。
伯努利分布：是一个离散型概率分布，若成功，则随机变量取值1；若失败，随机变量取值为0。成功概率记为p，失败为q = 1-p。


在逻辑回归中，既然假设了数据分布服从伯努利分布，那就存在一个成功和失败，对应二分类问题就是正类和负类，那么就应该有一个样本为正类的概率，和样本为负类的概率。假设有n个独立训练样本{（x1,y1）,（x2,y2）,（x3,y3）...,y={0,1}}，那每一个观察到的样本（xi,yi）出现的概率是


我们把线性回归简写为：

更一般的

引入sigmod函数

得到



若将y视为样本x的正例的可能性，1-y视为样本x的反例的可能性，两者的比值y/1-y称为“几率”，反映了x作为正例的相对可能性





以梯度下降为例

代价函数即为1/n的对数似然函数：





线性回归模型的偏回归系数通过最小二乘法（OLS）实现的，关于最小二乘法的使用是有一些假设前提的，具体是：
自变量与因变量之间存在线性关系；
自变量之间不存在多重共线性；
回归模型的残差服从正态分布；
回归模型的残差满足方差齐性（即方差为某个固定值）；
回归模型的残差之间互相独立；
线性回归模型对异常值是非常敏感的，模型预测的结果非常容易受到异常值的影响，所以，还需要对原始数据进行异常点识别和处理。
自变量和因变量间的相关系数：
变量之间不存在线性关系并不代表不存在任何关系，可能是二次函数关系、对数关系等，所以一般还需要进行检验和变量转换。
多重共线性检验：
如果模型的自变量之间存在严重的多重共线性的话，会产生什么后果呢?
导致最小二乘法OLS估计量可能无效；
增大最小二乘法OLS估计量的方差；
变量的显著性检验将失去意义；
模型缺乏稳定性；
所以，多重共线性检验就显得非常重要了，关于多重共线性的检验可以使用方差膨胀因子(VIF)来鉴定，如果VIF大于10，则说明变量存在多重共线性。一旦发现变量之间存在多重共线性的话，可以考虑删除变量和重新选择模型（岭回归法）。
异常点检测：
计算模型的RMSE值
fit = sm.formula.ols( 'y~x',data = data).fit()
fit.summary()
pred = fit.predict()np.sqrt(mean_squared_error(data.y, pred))
逻辑回归计算特征的重要性：
变量贡献率，反应各自变量对因变量影响程度的相对大小，计算步骤如下：
 1. 对所有自变量标准化；
 2. 对标准化后的自变量建逻辑回归模型，取各变量回归系数的绝对值；
 3. 计算各变量回归系数绝对值的占比即为特征贡献率。

3.线性判别分析
提问：
1.LR和线性回归的区别与联系？
逻辑回归和线性回归都是广义上的线性回归，逻辑回归是以线性回归为理论支持的。
经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数；线性回归在整个实数域范围内进行预测，而逻辑回归在【0,1】之间预测，线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题；目标函数也因此从差平方和函数变为对数损失函数；若要求多元分类,就要把sigmoid换成softmax了。
2.关于LR：
一般都是用梯度下降法来求解，梯度下降又有随机梯度下降，批梯度下降，small batch 梯度下降三种方式：
简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。
随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。
小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。
加分项，看你了不了解诸如Adam，动量法等优化方法，因为上述方法其实还有两个致命的问题：
第一个是如何对模型选择合适的学习率。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是学习到后面的时候，参数和最优解已经隔的比较近了，你还保持最初的学习率，容易越过最优点，在最优点附近来回振荡，通俗一点说，就很容易学过头了，跑偏了。
第二个是如何对参数选择合适的学习率。在实践中，对每个参数都保持的同样的学习率也是很不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。
3.逻辑回归为什么用极大似然函数作为损失函数？
一般和平方损失函数（最小二乘法）拿来比较，因为线性回归用的就是平方损失函数，原因就是平方损失函数加上sigmoid的函数将会是一个非凸的函数，不易求解，会得到局部解，用对数似然函数得到高阶连续可导凸函数，可以得到最优解。
其次，是因为对数损失函数更新起来很快，因为只和x，y有关，和sigmoid本身的梯度无关。
4.逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？
先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。
但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。
如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。
5.为什么我们还是会在训练的过程当中将高度相关的特征去掉
去掉高度相关的特征会让模型的可解释性更好。
可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。
6. 逻辑回归的优缺点总结
优点：
形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
资源占用小,尤其是内存。因为只需要存储各个维度的特征值。
方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cut off，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。
缺点：
准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。
7.LR如何解决线性不可分问题？
逻辑回归本质上是一个线性模型，但是，这不意味着只有线性可分的数据能通过LR求解，实际上，我们可以通过2种方式帮助LR实现：
（1）利用特殊核函数，对特征进行变换：把低维空间转换到高维空间，而在低维空间不可分的数据，到高维空间中线性可分的几率会高一些。
（2）扩展LR算法，提出FM算法。
使用核函数（特征组合映射）
针对线性不可分的数据集，可以尝试对给定的两个feature做一个多项式特征的映射，例如：

下面两个图的对比说明了线性分类曲线和非线性分类曲线（通过特征映射）

左图是一个线性可分的数据集，右图在原始空间中线性不可分，但是利用核函数，对特征转换 [x1,x2]=>[x1,x2,x21,x22,x1x2]
后的空间是线性可分的，对应的原始空间中分类边界为一条类椭圆曲线



在LR中，我们可以通过在基本线性回归模型的基础上引入交叉项，来实现非线性分类，如下：

但是这种直接在交叉项xixj的前面加上交叉项系数wij的方式在稀疏数据的情况下存在一个很大的缺陷，即在对于观察样本中未出现交互的特征分量，不能对相应的参数进行估计。
即，在数据稀疏性普遍存在的实际应用场景中，二次项参数的训练是很困难的。其原因是每个参数wij的训练需要大量xi和xj都非零的样本；由于样本数据本来就比较稀疏，满足xi和xj都非零的样本将会非常少。训练样本的不足很容易导致参数wij不准确，最终将严重影响模型的性能。
one-hot编码处理分类变量，带来的另一个问题是特征空间变大。同样以上面淘宝上的item为例，将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间一下子暴增一百万。在工业界，很少直接将连续值（eg.年龄特征）作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给LR。（？？？我表示怀疑）

8.LR为什么要对连续数值特征进行离散化？
离散特征的增加和减少都很容易，易于模型的快速迭代；
稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。
使用FM模型
另一种方法是对LR进行扩展，因子分解机（Factorization Machine，FM）是对LR算法的扩展。FM模型是一种基于矩阵分解的机器学习模型，对于稀疏数据具有很好的学习能力；
对于因子分解机FM来说，最大的特点是对于稀疏的数据具有很好的学习能力。
FM解决了LR泛化能力弱的问题，其目标函数如下所示：





9.LR与SVM的联系与区别：
联系：
1、LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题）
2、两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。
区别：
1、LR是参数模型[逻辑回归是假设y服从[Bernoulli分布]，SVM是非参数模型，LR对异常值更敏感。
2、从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
3、SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。
4、逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
5、logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。
10.如何选择LR与SVM？
非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。
如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。
模型复杂度：SVM支持核函数，可处理线性非线性问题;LR模型简单，训练速度快，适合处理线性问题;
损失函数：SVM hinge loss; LR L2正则化; adaboost 指数损失
数据敏感度：SVM添加容忍度对outlier不敏感，只关心支持向量，且需要先做归一化; LR对远点敏感
数据量：数据量大就用LR，数据量小且特征少就用SVM非线性核
11.什么是参数模型（LR）与非参数模型（SVM）？
在统计学中，参数模型通常假设总体（随机变量）服从某一个分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。

本文参考：
西瓜书
https://blog.csdn.net/xiazdong/article/details/7950084
https://www.cnblogs.com/mengnan/p/9307642.html
https://mp.weixin.qq.com/s/Mdn9yiT20oFhyuFLyd6YnA
https://www.jianshu.com/p/fa411ffb5490
